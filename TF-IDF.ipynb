{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e36c4468",
   "metadata": {},
   "source": [
    "# Importing Skills document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "566cbda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "746913e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 8\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "file_docs = []\n",
    "\n",
    "with open ('Resume_Scoring_NLP/Data Anlyst.txt',encoding=\"utf8\") as f:\n",
    "#     print(f.read())\n",
    "    tokens = sent_tokenize(f.read())\n",
    "    for line in tokens:\n",
    "        file_docs.append(line)\n",
    "\n",
    "print(\"Number of documents:\",len(file_docs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d251f440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Job description\\nOur Client is Headquartered in London UK and developing an Ai powered enterprise platform that enables insurers the flexibility to scale capacity as needed, safely, securely, efficiently and at speed.',\n",
       " 'Intelligent and flexible, tool understands global insurance processing challenges, and seamlessly solves them.',\n",
       " 'A smart and powerful modular SaaS platform that is built on robust technology, substantially minimizing time and money, whilst maximizing speed to market.',\n",
       " 'key skills :\\n\\n- Advanced Excel, word, PDF skills\\n\\n- Ability to work quickly manipulating and analyzing data\\n\\n- Ideally from a STEM background\\n\\n- Genuine interest in Data and Artificial intelligence\\n\\n- Python, C++ or VBA or SQL server experience is beneficial\\n\\n- Keen interest in automation artificial intelligence or data science essential\\n\\n- Italian Language is must.',\n",
       " '- 0-2 years experience\\n\\n- Until the Covid situation settles Work location is going to be remote .',\n",
       " '- Candidates must have ability to work from home and also in a position to use their personal laptop during this period.',\n",
       " 'RoleData:Analyst\\nIndustry Type:IT Services & Consulting\\nFunctional Area:Data Science & Analytics\\nEmployment Type:Full Time, Permanent\\nRole Category:Business Intelligence & Analytics\\nEducation\\nUG :B.Tech/B.E.',\n",
       " 'in Any Specialization, BCA in Any Specialization, Any Graduate, B.Sc in Any Specialization\\nPG :Any Postgraduate\\nDoctorate :Doctorate Not Required\\nKey Skills\\nPython Data Science C++ VBA Artificial Intelligence Data Analyst SQL Server STEM']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4181e0d1",
   "metadata": {},
   "source": [
    "# Preprcessing the document\n",
    "### It will convert the document into a list of lowercase tokens, ignoring tokens that are too short or too long. Also removed stop words. using `nltk.corpus.stopwords.words('english')`\n",
    "\n",
    "# Lemmatization\n",
    "### It stems the word but makes sure that it does not lose its meaning.  Lemmatization has a pre-defined dictionary that stores the context of words and checks the word in the dictionary while diminishing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a02fe6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "def pre_pros1(file):    \n",
    "    pre_pros1 = [gensim.utils.simple_preprocess(wordnet_lemmatizer.lemmatize(text)) for text in file_docs if text not in nltk.corpus.stopwords.words('english')]\n",
    "    return pre_pros1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d281e550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kites ---> kite\n",
      "babies ---> baby\n",
      "dogs ---> dog\n",
      "flying ---> flying\n",
      "smiling ---> smiling\n",
      "driving ---> driving\n",
      "died ---> died\n",
      "tried ---> tried\n",
      "feet ---> foot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DollaR\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    " \n",
    "# Create WordNetLemmatizer object\n",
    "wnl = WordNetLemmatizer()\n",
    " \n",
    "# single word lemmatization examples\n",
    "list1 = ['kites', 'babies', 'dogs', 'flying', 'smiling', 'driving', 'died', 'tried', 'feet']\n",
    "for words in list1:\n",
    "    print(words + \" ---> \" + wnl.lemmatize(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "078c81df",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_pros1 = pre_pros1(file_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "aa4a8ab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['intelligent',\n",
       " 'and',\n",
       " 'flexible',\n",
       " 'tool',\n",
       " 'understands',\n",
       " 'global',\n",
       " 'insurance',\n",
       " 'processing',\n",
       " 'challenges',\n",
       " 'and',\n",
       " 'seamlessly',\n",
       " 'solves',\n",
       " 'them']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_pros1[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44d2e34",
   "metadata": {},
   "source": [
    "# Dictionary maps each word to a unique id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "510146b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(pre_pros1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b0da8b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id  0 ai\n",
      "id  1 an\n",
      "id  2 and\n",
      "id  3 as\n",
      "id  4 at\n",
      "id  5 capacity\n",
      "id  6 client\n",
      "id  7 description\n",
      "id  8 developing\n",
      "id  9 efficiently\n",
      "id  10 enables\n",
      "id  11 enterprise\n",
      "id  12 flexibility\n",
      "id  13 headquartered\n",
      "id  14 in\n",
      "id  15 insurers\n",
      "id  16 is\n",
      "id  17 job\n",
      "id  18 london\n",
      "id  19 needed\n",
      "id  20 our\n",
      "id  21 platform\n",
      "id  22 powered\n",
      "id  23 safely\n",
      "id  24 scale\n",
      "id  25 securely\n",
      "id  26 speed\n",
      "id  27 that\n",
      "id  28 the\n",
      "id  29 to\n",
      "id  30 uk\n",
      "id  31 challenges\n",
      "id  32 flexible\n",
      "id  33 global\n",
      "id  34 insurance\n",
      "id  35 intelligent\n",
      "id  36 processing\n",
      "id  37 seamlessly\n",
      "id  38 solves\n",
      "id  39 them\n",
      "id  40 tool\n",
      "id  41 understands\n",
      "id  42 built\n",
      "id  43 market\n",
      "id  44 maximizing\n",
      "id  45 minimizing\n",
      "id  46 modular\n",
      "id  47 money\n",
      "id  48 on\n",
      "id  49 powerful\n",
      "id  50 robust\n",
      "id  51 saas\n",
      "id  52 smart\n",
      "id  53 substantially\n",
      "id  54 technology\n",
      "id  55 time\n",
      "id  56 whilst\n",
      "id  57 ability\n",
      "id  58 advanced\n",
      "id  59 analyzing\n",
      "id  60 artificial\n",
      "id  61 automation\n",
      "id  62 background\n",
      "id  63 beneficial\n",
      "id  64 data\n",
      "id  65 essential\n",
      "id  66 excel\n",
      "id  67 experience\n",
      "id  68 from\n",
      "id  69 genuine\n",
      "id  70 ideally\n",
      "id  71 intelligence\n",
      "id  72 interest\n",
      "id  73 italian\n",
      "id  74 keen\n",
      "id  75 key\n",
      "id  76 language\n",
      "id  77 manipulating\n",
      "id  78 must\n",
      "id  79 or\n",
      "id  80 pdf\n",
      "id  81 python\n",
      "id  82 quickly\n",
      "id  83 science\n",
      "id  84 server\n",
      "id  85 skills\n",
      "id  86 sql\n",
      "id  87 stem\n",
      "id  88 vba\n",
      "id  89 word\n",
      "id  90 work\n",
      "id  91 be\n",
      "id  92 covid\n",
      "id  93 going\n",
      "id  94 location\n",
      "id  95 remote\n",
      "id  96 settles\n",
      "id  97 situation\n",
      "id  98 until\n",
      "id  99 years\n",
      "id  100 also\n",
      "id  101 candidates\n",
      "id  102 during\n",
      "id  103 have\n",
      "id  104 home\n",
      "id  105 laptop\n",
      "id  106 period\n",
      "id  107 personal\n",
      "id  108 position\n",
      "id  109 their\n",
      "id  110 this\n",
      "id  111 use\n",
      "id  112 analyst\n",
      "id  113 analytics\n",
      "id  114 area\n",
      "id  115 business\n",
      "id  116 category\n",
      "id  117 consulting\n",
      "id  118 education\n",
      "id  119 employment\n",
      "id  120 full\n",
      "id  121 functional\n",
      "id  122 industry\n",
      "id  123 it\n",
      "id  124 permanent\n",
      "id  125 role\n",
      "id  126 roledata\n",
      "id  127 services\n",
      "id  128 tech\n",
      "id  129 type\n",
      "id  130 ug\n",
      "id  131 any\n",
      "id  132 bca\n",
      "id  133 doctorate\n",
      "id  134 graduate\n",
      "id  135 not\n",
      "id  136 pg\n",
      "id  137 postgraduate\n",
      "id  138 required\n",
      "id  139 sc\n",
      "id  140 specialization\n"
     ]
    }
   ],
   "source": [
    "for i in dictionary:\n",
    "    print('id ',i, dictionary[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7f5ca9",
   "metadata": {},
   "source": [
    "# Creating a bag of words\n",
    "### It contains the word id and its frequency in each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa619d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1e+03 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "corpus = [dictionary.doc2bow(doc) for doc in pre_pros1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5d0e8eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word  ai id  0 freq  1\n",
      "word  an id  1 freq  1\n",
      "word  and id  2 freq  2\n",
      "word  as id  3 freq  1\n",
      "word  at id  4 freq  1\n",
      "word  capacity id  5 freq  1\n",
      "word  client id  6 freq  1\n",
      "word  description id  7 freq  1\n",
      "word  developing id  8 freq  1\n",
      "word  efficiently id  9 freq  1\n",
      "word  enables id  10 freq  1\n",
      "word  enterprise id  11 freq  1\n",
      "word  flexibility id  12 freq  1\n",
      "word  headquartered id  13 freq  1\n",
      "word  in id  14 freq  1\n",
      "word  insurers id  15 freq  1\n",
      "word  is id  16 freq  1\n",
      "word  job id  17 freq  1\n",
      "word  london id  18 freq  1\n",
      "word  needed id  19 freq  1\n",
      "word  our id  20 freq  1\n",
      "word  platform id  21 freq  1\n",
      "word  powered id  22 freq  1\n",
      "word  safely id  23 freq  1\n",
      "word  scale id  24 freq  1\n",
      "word  securely id  25 freq  1\n",
      "word  speed id  26 freq  1\n",
      "word  that id  27 freq  1\n",
      "word  the id  28 freq  1\n",
      "word  to id  29 freq  1\n",
      "word  uk id  30 freq  1\n",
      "word  and id  2 freq  2\n",
      "word  challenges id  31 freq  1\n",
      "word  flexible id  32 freq  1\n",
      "word  global id  33 freq  1\n",
      "word  insurance id  34 freq  1\n",
      "word  intelligent id  35 freq  1\n",
      "word  processing id  36 freq  1\n",
      "word  seamlessly id  37 freq  1\n",
      "word  solves id  38 freq  1\n",
      "word  them id  39 freq  1\n",
      "word  tool id  40 freq  1\n",
      "word  understands id  41 freq  1\n",
      "word  and id  2 freq  2\n",
      "word  is id  16 freq  1\n",
      "word  platform id  21 freq  1\n",
      "word  speed id  26 freq  1\n",
      "word  that id  27 freq  1\n",
      "word  to id  29 freq  1\n",
      "word  built id  42 freq  1\n",
      "word  market id  43 freq  1\n",
      "word  maximizing id  44 freq  1\n",
      "word  minimizing id  45 freq  1\n",
      "word  modular id  46 freq  1\n",
      "word  money id  47 freq  1\n",
      "word  on id  48 freq  1\n",
      "word  powerful id  49 freq  1\n",
      "word  robust id  50 freq  1\n",
      "word  saas id  51 freq  1\n",
      "word  smart id  52 freq  1\n",
      "word  substantially id  53 freq  1\n",
      "word  technology id  54 freq  1\n",
      "word  time id  55 freq  1\n",
      "word  whilst id  56 freq  1\n",
      "word  and id  2 freq  2\n",
      "word  in id  14 freq  2\n",
      "word  is id  16 freq  2\n",
      "word  to id  29 freq  1\n",
      "word  ability id  57 freq  1\n",
      "word  advanced id  58 freq  1\n",
      "word  analyzing id  59 freq  1\n",
      "word  artificial id  60 freq  2\n",
      "word  automation id  61 freq  1\n",
      "word  background id  62 freq  1\n",
      "word  beneficial id  63 freq  1\n",
      "word  data id  64 freq  3\n",
      "word  essential id  65 freq  1\n",
      "word  excel id  66 freq  1\n",
      "word  experience id  67 freq  1\n",
      "word  from id  68 freq  1\n",
      "word  genuine id  69 freq  1\n",
      "word  ideally id  70 freq  1\n",
      "word  intelligence id  71 freq  2\n",
      "word  interest id  72 freq  2\n",
      "word  italian id  73 freq  1\n",
      "word  keen id  74 freq  1\n",
      "word  key id  75 freq  1\n",
      "word  language id  76 freq  1\n",
      "word  manipulating id  77 freq  1\n",
      "word  must id  78 freq  1\n",
      "word  or id  79 freq  3\n",
      "word  pdf id  80 freq  1\n",
      "word  python id  81 freq  1\n",
      "word  quickly id  82 freq  1\n",
      "word  science id  83 freq  1\n",
      "word  server id  84 freq  1\n",
      "word  skills id  85 freq  2\n",
      "word  sql id  86 freq  1\n",
      "word  stem id  87 freq  1\n",
      "word  vba id  88 freq  1\n",
      "word  word id  89 freq  1\n",
      "word  work id  90 freq  1\n",
      "word  is id  16 freq  1\n",
      "word  the id  28 freq  1\n",
      "word  to id  29 freq  1\n",
      "word  experience id  67 freq  1\n",
      "word  work id  90 freq  1\n",
      "word  be id  91 freq  1\n",
      "word  covid id  92 freq  1\n",
      "word  going id  93 freq  1\n",
      "word  location id  94 freq  1\n",
      "word  remote id  95 freq  1\n",
      "word  settles id  96 freq  1\n",
      "word  situation id  97 freq  1\n",
      "word  until id  98 freq  1\n",
      "word  years id  99 freq  1\n",
      "word  and id  2 freq  1\n",
      "word  in id  14 freq  1\n",
      "word  to id  29 freq  2\n",
      "word  ability id  57 freq  1\n",
      "word  from id  68 freq  1\n",
      "word  must id  78 freq  1\n",
      "word  work id  90 freq  1\n",
      "word  also id  100 freq  1\n",
      "word  candidates id  101 freq  1\n",
      "word  during id  102 freq  1\n",
      "word  have id  103 freq  1\n",
      "word  home id  104 freq  1\n",
      "word  laptop id  105 freq  1\n",
      "word  period id  106 freq  1\n",
      "word  personal id  107 freq  1\n",
      "word  position id  108 freq  1\n",
      "word  their id  109 freq  1\n",
      "word  this id  110 freq  1\n",
      "word  use id  111 freq  1\n",
      "word  time id  55 freq  1\n",
      "word  data id  64 freq  1\n",
      "word  intelligence id  71 freq  1\n",
      "word  science id  83 freq  1\n",
      "word  analyst id  112 freq  1\n",
      "word  analytics id  113 freq  2\n",
      "word  area id  114 freq  1\n",
      "word  business id  115 freq  1\n",
      "word  category id  116 freq  1\n",
      "word  consulting id  117 freq  1\n",
      "word  education id  118 freq  1\n",
      "word  employment id  119 freq  1\n",
      "word  full id  120 freq  1\n",
      "word  functional id  121 freq  1\n",
      "word  industry id  122 freq  1\n",
      "word  it id  123 freq  1\n",
      "word  permanent id  124 freq  1\n",
      "word  role id  125 freq  1\n",
      "word  roledata id  126 freq  1\n",
      "word  services id  127 freq  1\n",
      "word  tech id  128 freq  1\n",
      "word  type id  129 freq  2\n",
      "word  ug id  130 freq  1\n",
      "word  in id  14 freq  3\n",
      "word  artificial id  60 freq  1\n",
      "word  data id  64 freq  2\n",
      "word  intelligence id  71 freq  1\n",
      "word  key id  75 freq  1\n",
      "word  python id  81 freq  1\n",
      "word  science id  83 freq  1\n",
      "word  server id  84 freq  1\n",
      "word  skills id  85 freq  1\n",
      "word  sql id  86 freq  1\n",
      "word  stem id  87 freq  1\n",
      "word  vba id  88 freq  1\n",
      "word  analyst id  112 freq  1\n",
      "word  any id  131 freq  5\n",
      "word  bca id  132 freq  1\n",
      "word  doctorate id  133 freq  2\n",
      "word  graduate id  134 freq  1\n",
      "word  not id  135 freq  1\n",
      "word  pg id  136 freq  1\n",
      "word  postgraduate id  137 freq  1\n",
      "word  required id  138 freq  1\n",
      "word  sc id  139 freq  1\n",
      "word  specialization id  140 freq  3\n"
     ]
    }
   ],
   "source": [
    "for i in corpus:\n",
    "    for j in i:\n",
    "        print('word ',dictionary[j[0]], 'id ',j[0], 'freq ',j[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c1d52daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ai 1\n",
      "an 1\n",
      "and 2\n",
      "as 1\n",
      "at 1\n",
      "capacity 1\n",
      "client 1\n",
      "description 1\n",
      "developing 1\n",
      "efficiently 1\n",
      "enables 1\n",
      "enterprise 1\n",
      "flexibility 1\n",
      "headquartered 1\n",
      "in 1\n",
      "insurers 1\n",
      "is 1\n",
      "job 1\n",
      "london 1\n",
      "needed 1\n",
      "our 1\n",
      "platform 1\n",
      "powered 1\n",
      "safely 1\n",
      "scale 1\n",
      "securely 1\n",
      "speed 1\n",
      "that 1\n",
      "the 1\n",
      "to 1\n",
      "uk 1\n",
      "---\n",
      "and 2\n",
      "challenges 1\n",
      "flexible 1\n",
      "global 1\n",
      "insurance 1\n",
      "intelligent 1\n",
      "processing 1\n",
      "seamlessly 1\n",
      "solves 1\n",
      "them 1\n",
      "tool 1\n",
      "understands 1\n",
      "---\n",
      "and 2\n",
      "is 1\n",
      "platform 1\n",
      "speed 1\n",
      "that 1\n",
      "to 1\n",
      "built 1\n",
      "market 1\n",
      "maximizing 1\n",
      "minimizing 1\n",
      "modular 1\n",
      "money 1\n",
      "on 1\n",
      "powerful 1\n",
      "robust 1\n",
      "saas 1\n",
      "smart 1\n",
      "substantially 1\n",
      "technology 1\n",
      "time 1\n",
      "whilst 1\n",
      "---\n",
      "and 2\n",
      "in 2\n",
      "is 2\n",
      "to 1\n",
      "ability 1\n",
      "advanced 1\n",
      "analyzing 1\n",
      "artificial 2\n",
      "automation 1\n",
      "background 1\n",
      "beneficial 1\n",
      "data 3\n",
      "essential 1\n",
      "excel 1\n",
      "experience 1\n",
      "from 1\n",
      "genuine 1\n",
      "ideally 1\n",
      "intelligence 2\n",
      "interest 2\n",
      "italian 1\n",
      "keen 1\n",
      "key 1\n",
      "language 1\n",
      "manipulating 1\n",
      "must 1\n",
      "or 3\n",
      "pdf 1\n",
      "python 1\n",
      "quickly 1\n",
      "science 1\n",
      "server 1\n",
      "skills 2\n",
      "sql 1\n",
      "stem 1\n",
      "vba 1\n",
      "word 1\n",
      "work 1\n",
      "---\n",
      "is 1\n",
      "the 1\n",
      "to 1\n",
      "experience 1\n",
      "work 1\n",
      "be 1\n",
      "covid 1\n",
      "going 1\n",
      "location 1\n",
      "remote 1\n",
      "settles 1\n",
      "situation 1\n",
      "until 1\n",
      "years 1\n",
      "---\n",
      "and 1\n",
      "in 1\n",
      "to 2\n",
      "ability 1\n",
      "from 1\n",
      "must 1\n",
      "work 1\n",
      "also 1\n",
      "candidates 1\n",
      "during 1\n",
      "have 1\n",
      "home 1\n",
      "laptop 1\n",
      "period 1\n",
      "personal 1\n",
      "position 1\n",
      "their 1\n",
      "this 1\n",
      "use 1\n",
      "---\n",
      "time 1\n",
      "data 1\n",
      "intelligence 1\n",
      "science 1\n",
      "analyst 1\n",
      "analytics 2\n",
      "area 1\n",
      "business 1\n",
      "category 1\n",
      "consulting 1\n",
      "education 1\n",
      "employment 1\n",
      "full 1\n",
      "functional 1\n",
      "industry 1\n",
      "it 1\n",
      "permanent 1\n",
      "role 1\n",
      "roledata 1\n",
      "services 1\n",
      "tech 1\n",
      "type 2\n",
      "ug 1\n",
      "---\n",
      "in 3\n",
      "artificial 1\n",
      "data 2\n",
      "intelligence 1\n",
      "key 1\n",
      "python 1\n",
      "science 1\n",
      "server 1\n",
      "skills 1\n",
      "sql 1\n",
      "stem 1\n",
      "vba 1\n",
      "analyst 1\n",
      "any 5\n",
      "bca 1\n",
      "doctorate 2\n",
      "graduate 1\n",
      "not 1\n",
      "pg 1\n",
      "postgraduate 1\n",
      "required 1\n",
      "sc 1\n",
      "specialization 3\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for i in corpus:\n",
    "    for j in i:\n",
    "        print(dictionary[j[0]], j[1])\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae0b30e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dictionary.keys())[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1923c20",
   "metadata": {},
   "source": [
    "# TF-IDF (term frequency-inverse document frequency) \n",
    "### It is a statistical measure that evaluates how relevant a word is to a document in a collection of documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8744169c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.99 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tf_idf = gensim.models.TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a74bd6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ai 0.2\n",
      "an 0.2\n",
      "and 0.09\n",
      "as 0.2\n",
      "at 0.2\n",
      "capacity 0.2\n",
      "client 0.2\n",
      "description 0.2\n",
      "developing 0.2\n",
      "efficiently 0.2\n",
      "enables 0.2\n",
      "enterprise 0.2\n",
      "flexibility 0.2\n",
      "headquartered 0.2\n",
      "in 0.07\n",
      "insurers 0.2\n",
      "is 0.07\n",
      "job 0.2\n",
      "london 0.2\n",
      "needed 0.2\n",
      "our 0.2\n",
      "platform 0.13\n",
      "powered 0.2\n",
      "safely 0.2\n",
      "scale 0.2\n",
      "securely 0.2\n",
      "speed 0.13\n",
      "that 0.13\n",
      "the 0.13\n",
      "to 0.04\n",
      "uk 0.2\n",
      "and 0.14\n",
      "challenges 0.3\n",
      "flexible 0.3\n",
      "global 0.3\n",
      "insurance 0.3\n",
      "intelligent 0.3\n",
      "processing 0.3\n",
      "seamlessly 0.3\n",
      "solves 0.3\n",
      "them 0.3\n",
      "tool 0.3\n",
      "understands 0.3\n",
      "and 0.11\n",
      "is 0.08\n",
      "platform 0.17\n",
      "speed 0.17\n",
      "that 0.17\n",
      "to 0.06\n",
      "built 0.25\n",
      "market 0.25\n",
      "maximizing 0.25\n",
      "minimizing 0.25\n",
      "modular 0.25\n",
      "money 0.25\n",
      "on 0.25\n",
      "powerful 0.25\n",
      "robust 0.25\n",
      "saas 0.25\n",
      "smart 0.25\n",
      "substantially 0.25\n",
      "technology 0.25\n",
      "time 0.17\n",
      "whilst 0.25\n",
      "and 0.07\n",
      "in 0.1\n",
      "is 0.1\n",
      "to 0.04\n",
      "ability 0.1\n",
      "advanced 0.16\n",
      "analyzing 0.16\n",
      "artificial 0.21\n",
      "automation 0.16\n",
      "background 0.16\n",
      "beneficial 0.16\n",
      "data 0.22\n",
      "essential 0.16\n",
      "excel 0.16\n",
      "experience 0.1\n",
      "from 0.1\n",
      "genuine 0.16\n",
      "ideally 0.16\n",
      "intelligence 0.15\n",
      "interest 0.31\n",
      "italian 0.16\n",
      "keen 0.16\n",
      "key 0.1\n",
      "language 0.16\n",
      "manipulating 0.16\n",
      "must 0.1\n",
      "or 0.47\n",
      "pdf 0.16\n",
      "python 0.1\n",
      "quickly 0.16\n",
      "science 0.07\n",
      "server 0.1\n",
      "skills 0.21\n",
      "sql 0.1\n",
      "stem 0.1\n",
      "vba 0.1\n",
      "word 0.16\n",
      "work 0.07\n",
      "is 0.1\n",
      "the 0.21\n",
      "to 0.07\n",
      "experience 0.21\n",
      "work 0.15\n",
      "be 0.31\n",
      "covid 0.31\n",
      "going 0.31\n",
      "location 0.31\n",
      "remote 0.31\n",
      "settles 0.31\n",
      "situation 0.31\n",
      "until 0.31\n",
      "years 0.31\n",
      "and 0.06\n",
      "in 0.09\n",
      "to 0.12\n",
      "ability 0.18\n",
      "from 0.18\n",
      "must 0.18\n",
      "work 0.13\n",
      "also 0.27\n",
      "candidates 0.27\n",
      "during 0.27\n",
      "have 0.27\n",
      "home 0.27\n",
      "laptop 0.27\n",
      "period 0.27\n",
      "personal 0.27\n",
      "position 0.27\n",
      "their 0.27\n",
      "this 0.27\n",
      "use 0.27\n",
      "time 0.13\n",
      "data 0.09\n",
      "intelligence 0.09\n",
      "science 0.09\n",
      "analyst 0.13\n",
      "analytics 0.4\n",
      "area 0.2\n",
      "business 0.2\n",
      "category 0.2\n",
      "consulting 0.2\n",
      "education 0.2\n",
      "employment 0.2\n",
      "full 0.2\n",
      "functional 0.2\n",
      "industry 0.2\n",
      "it 0.2\n",
      "permanent 0.2\n",
      "role 0.2\n",
      "roledata 0.2\n",
      "services 0.2\n",
      "tech 0.2\n",
      "type 0.4\n",
      "ug 0.2\n",
      "in 0.14\n",
      "artificial 0.09\n",
      "data 0.13\n",
      "intelligence 0.07\n",
      "key 0.09\n",
      "python 0.09\n",
      "science 0.07\n",
      "server 0.09\n",
      "skills 0.09\n",
      "sql 0.09\n",
      "stem 0.09\n",
      "vba 0.09\n",
      "analyst 0.09\n",
      "any 0.7\n",
      "bca 0.14\n",
      "doctorate 0.28\n",
      "graduate 0.14\n",
      "not 0.14\n",
      "pg 0.14\n",
      "postgraduate 0.14\n",
      "required 0.14\n",
      "sc 0.14\n",
      "specialization 0.42\n"
     ]
    }
   ],
   "source": [
    "for doc in tf_idf[corpus]:\n",
    "    for id_, freq in doc:\n",
    "        print(dictionary[id_], np.around(freq, decimals=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5336f7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sims = gensim.similarities.Similarity('xx',tf_idf[corpus], num_features=len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a41c3d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath=r'C:\\Users\\DollaR\\OneDrive\\Documents\\HR Analytics\\Resume_Scoring_NLP\\Resumes'\n",
    "#Path for the files\n",
    "onlyfiles = [os.path.join(mypath, f) for f in os.listdir(mypath) if os.path.isfile(os.path.join(mypath, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "370dd497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import PyPDF2\n",
    "def pdfextract(file):\n",
    "    pdf_file = open(file, 'rb')\n",
    "    read_pdf = PyPDF2.PdfFileReader(pdf_file)\n",
    "    number_of_pages = read_pdf.getNumPages()\n",
    "    c = collections.Counter(range(number_of_pages))\n",
    "    for i in c:\n",
    "        #page\n",
    "        page = read_pdf.getPage(i)\n",
    "        page_content = page.extractText()\n",
    "    return(page_content.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b29cba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cand_name = []\n",
    "cand_score = []\n",
    "for file_path in onlyfiles:\n",
    "    str1 = str(pdfextract(file_path))\n",
    "    rsm1 = nltk.tokenize.regexp_tokenize(str1, r\"\\w+\")\n",
    "    new_rsm1 = []\n",
    "    for word in rsm1:\n",
    "        if word not in nltk.corpus.stopwords.words('english'):\n",
    "            new_rsm1.append(wordnet_lemmatizer.lemmatize(word).lower())\n",
    "    query_doc_bow = dictionary.doc2bow(new_rsm1)\n",
    "    query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "#     print(sims[query_doc_tf_idf])\n",
    "    score = np.average(sims[query_doc_tf_idf])*100\n",
    "    cand = file_path.split('\\\\')[-1].split('.')[0]   \n",
    "    cand_name.append(cand)\n",
    "    cand_score.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2dce39cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result = pd.DataFrame({'cand_name':cand_name, 'cand_score':cand_score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddd0b57f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cand_name</th>\n",
       "      <th>cand_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AmanSharma</td>\n",
       "      <td>1.310956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chandler</td>\n",
       "      <td>0.785071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MeghnaLohani</td>\n",
       "      <td>1.010100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Phoebe Buffay</td>\n",
       "      <td>0.964467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VAISHALI BIJOY</td>\n",
       "      <td>0.709491</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        cand_name  cand_score\n",
       "0      AmanSharma    1.310956\n",
       "1        Chandler    0.785071\n",
       "2    MeghnaLohani    1.010100\n",
       "3   Phoebe Buffay    0.964467\n",
       "4  VAISHALI BIJOY    0.709491"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c695f4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cand_name</th>\n",
       "      <th>cand_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AmanSharma</td>\n",
       "      <td>9.007896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chandler</td>\n",
       "      <td>5.281189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MeghnaLohani</td>\n",
       "      <td>7.949153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Phoebe Buffay</td>\n",
       "      <td>5.281189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VAISHALI BIJOY</td>\n",
       "      <td>3.734365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        cand_name  cand_score\n",
       "0      AmanSharma    9.007896\n",
       "1        Chandler    5.281189\n",
       "2    MeghnaLohani    7.949153\n",
       "3   Phoebe Buffay    5.281189\n",
       "4  VAISHALI BIJOY    3.734365"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195e8539",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
